Objective:
This document defines the reference architecture and behavioral standards for Kubernetes Scaling Jobs (KSJ) used for asynchronous message processing in our platform. It outlines how we prefer a KSJ to function, handles edge cases, and defines default configurations for KEDA, SQS, and job lifecycle.


Summary
1) A Kubernetes Scaling Job (KSJ) is designed to:
2) Dynamically scale based on queue depth using KEDA
3) Process messages from SQS
4) Ensure messages are reliably handled, retried if needed, and never lost silently
5) Provide observability through structured logs
6) Be resilient to failures and handle graceful shutdowns


updated summary:
This KSJ sample app is designed to:

1) Process a single message from SQS per job execution
2) Use KEDA to dynamically scale jobs based on SQS queue depth
3) Extend message visibility timeout dynamically during long-running processing
4) Handle SIGTERM signals gracefully to avoid silent message loss
5) Provide structured logs compatible with Fluent Bit ‚Üí Splunk or other observability pipelines

Reference Workflow:
***
+-------------+ +------------+ +----------------+
| Developer/API | -----> | S3 | -----> | SQS |
+-------------+ +------------+ +----------------+
|
v
+---------------------------+
| KEDA ScaledObject |
| (SQS Trigger) |
+---------------------------+
|
v
+--------------------------+
| K8s ScaledJob (KSJ)|
+--------------------------+
|
+------------------------+-------------------+-------------------------+
| | |
Read SQS Msg Execute Business Logic Log + Retry Policy
| | |
On Success On Success On Failure
| | |
Delete SQS Msg Terminate Pod Let message timeout (remain in
queue)



    +------------------------------+
          |     Kubernetes Scaling Job   |
          |    (KSJ Worker Container)    |
          +------------------------------+
                            |
                            v
             +------------------------------+
             |  Read Message from SQS       |  ‚Üê Visibility Timeout Starts
             +------------------------------+
                            |
                            v
             +------------------------------+
             |  Start Background Thread     |
             |  to Extend Visibility        |
             +------------------------------+
                            |
                            v
             +------------------------------+
             |  Execute Job Logic           |
             |  (e.g. process order, image) |
             +------------------------------+
                            |
            / \                             \
           /   \                             \ SIGTERM received?
   Success     Failure                        ‚Üí Set shutdown_event
     |             |                          ‚Üí Do NOT delete message
     |             v
     |     +-----------------------------+
     |     |  Log Error, Let Timeout Expire
     |     +-----------------------------+
     |
     v
+-----------------------------+
|   Delete Message from SQS   |
+-----------------------------+

Implementation Guidelines
1. Message Visibility Timeout:
The sample KSJ worker uses Amazon SQS visibility timeout to ensure messages are not processed by more than one worker simultaneously. This section outlines how timeouts are configured and dynamically extended during job execution.
| Feature               | Recommendation                                                        |
| --------------------- | --------------------------------------------------------------------- |
| **Initial Timeout**   | 30‚Äì60 seconds (via `ReceiveMessage`)                                  |
| **Dynamic Extension** | Use if job duration is variable or long                               |
| **Maximum Timeout**   | Should match max expected processing time (e.g., 10 minutes)          |
| **On Failure**        | Don‚Äôt delete message ‚Äì let it become visible again after timeout      |


Initial Timeout (30 seconds)

```VISIBILITY_TIMEOUT = int(os.getenv("INITIAL_VISIBILITY_TIMEOUT", 30))  # Default 30s

response = sqs.receive_message(
    QueueUrl=QUEUE_URL,
    MaxNumberOfMessages=1,
    WaitTimeSeconds=10,
    VisibilityTimeout=VISIBILITY_TIMEOUT  # <-- Initial visibility timeout
)
```
Recommendation:
Set the initial visibility timeout between 30‚Äì60 seconds ‚Äî long enough for most short jobs, but short enough to allow retries if the job crashes quickly.

Dynamic Extension:
```
EXTEND_INTERVAL = int(os.getenv("VISIBILITY_EXTENSION_INTERVAL", 10))  # every 10s
EXTEND_BY = int(os.getenv("EXTEND_BY", 30))  # extend by 30s

def extend_visibility_timeout():
    while not shutdown_event.is_set():
        time.sleep(EXTEND_INTERVAL)
        if receipt_handle:
            try:
                logging.info(f"Extending visibility timeout by {EXTEND_BY}s")
                sqs.change_message_visibility(
                    QueueUrl=QUEUE_URL,
                    ReceiptHandle=receipt_handle,
                    VisibilityTimeout=EXTEND_BY
                )
            except Exception as e:
                logging.error(f"Failed to extend visibility timeout: {e}")
```
A background thread runs every EXTEND_INTERVAL to extend the timeout while the job is still working.
Recommendation for Dynamic Extension:
We recommend dynamic visibility timeout only if:

1) Our job has variable processing times, and static timeouts would be inefficient

2) We want faster recovery from job crashes (e.g., via retry within 30s instead of waiting full max timeout)

3) We are not using FIFO queues with deduplication delay issues

4) If Our job always takes 60 seconds, we may skip dynamic extension and just set VisibilityTimeout=60.

On Failure ‚Äì Do Not Delete Message
```
try:
    process_message(message_body)

    if shutdown_event.is_set():
        logging.warning("Job interrupted. Message will return to queue.")
    else:
        logging.info("Deleting message from queue")
        sqs.delete_message(
            QueueUrl=QUEUE_URL,
            ReceiptHandle=receipt_handle
        )

except Exception as e:
    logging.error(f"Job failed with exception: {e}")
    # Message is NOT deleted ‚Äî becomes visible again after timeout
```
If the job crashes or is interrupted (SIGTERM), the message is left in SQS and reappears after timeout.



2) Read Message ‚Üí Process Message ‚Üí  Delete Message
following code snippet is just a sample app reference

 a. Read 1 Message from SQS
```
response = sqs.receive_message(
    QueueUrl=QUEUE_URL,
    MaxNumberOfMessages=1,  # üëà Reads only one message per job
    WaitTimeSeconds=10,
    VisibilityTimeout=VISIBILITY_TIMEOUT
)
```

This is the message retrieval step.

MaxNumberOfMessages=1: Ensures one job handles one message.

VisibilityTimeout: Ensures the message becomes invisible to others while the job is processing it.

Note: This makes the job scalable, isolated, and avoids concurrent processing of the same message.

b. Process the Message
following code snippet is just a sample app reference
```
def process_message(message_body, receipt_handle):
    logging.info(f"Started processing message: {message_body}")

    total_processing_time = 60  # Total simulated job time in seconds
    elapsed = 0

    while elapsed < total_processing_time and not shutdown_event.is_set():
        time.sleep(EXTEND_INTERVAL)  # ‚è±Ô∏è Sleep for configured interval
        elapsed += EXTEND_INTERVAL

        logging.info(f"Processed {elapsed}/{total_processing_time} seconds")

        try:
            logging.info(f"Extending visibility timeout by {EXTEND_BY}s")
            sqs.change_message_visibility(
                QueueUrl=QUEUE_URL,
                ReceiptHandle=receipt_handle,
                VisibilityTimeout=EXTEND_BY
            )
        except Exception as e:
            logging.error(f"Failed to extend visibility timeout: {e}")

    if shutdown_event.is_set():
        logging.warning("Processing was interrupted by shutdown.")
    else:
        logging.info("Finished processing message")

```

here we will be having our code logic.

| Config Var        | Usage in Function                                        |
| ----------------- | -------------------------------------------------------- |
| `EXTEND_INTERVAL` | Used as sleep duration in the loop ‚Äî how often to extend |
| `EXTEND_BY`       | Duration to extend visibility on each API call           |

c. Delete the Message (Only on Success)
```
if shutdown_event.is_set():
    logging.warning("Job interrupted. Message will return to queue.")
else:
    logging.info("Deleting message from queue")
    sqs.delete_message(
        QueueUrl=QUEUE_URL,
        ReceiptHandle=receipt_handle
    )
```


If processing completed successfully and job was not interrupted, the message is explicitly deleted from the queue.

This prevents it from being reprocessed.

Note: Only delete after successful processing.

3) Handling SIGTERM:
When Kubernetes terminates a pod (e.g., scaling down or job completion), it sends a SIGTERM signal to give the container a chance to shut down cleanly before forcibly killing it with SIGKILL.

Without handling SIGTERM:

> our worker might exit mid-processing

> The message might get lost or never reappear (if deleted too early)

Every long-running loop (like the message processor or visibility extender) checks shutdown_event.is_set():

```
while elapsed < total_processing_time and not shutdown_event.is_set():
```
As soon as SIGTERM is received, shutdown_event is set
```
while not shutdown_event.is_set():
```
All loops will gracefully break out, log exit, and stop processing

Our Logic should integrates graceful SIGTERM handling correctly:

> Catches the signal

> Signals all threads to stop

> Skips deleting the message on early termination

4)Logging 
All logs must be written to stdout

Use structured log format:

```
2025-06-23T10:00:00Z | INFO | Processing message: {messageId: abc123}
2025-06-23T10:00:15Z | WARN | SIGTERM received, exiting
2025-06-23T10:00:20Z | ERROR | Job failed with: TimeoutError
```
Required fields:

message_id, job_id, retry_count, duration, status, error

5) Keda Configurations:
This configuration defines how the KSJ sample app integrates with KEDA ScaledJob for scaling behavior.
These values are reference defaults for this sample pattern. Actual values should be adjusted based on your job‚Äôs behavior, retry policy, and cost/resource tradeoffs.

| Setting                       | Recommended Default             | Description                                                             |
| ----------------------------- | ------------------------------- | ------------------------------------------------------------------------|
| `pollingInterval`             | `10s`                           | Frequency at which KEDA checks SQS for pending messages                 |
| `minReplicaCount`             | `0`                             |Zero idle pods; scale from zero                                          |
| `maxReplicaCount`             | Set to match queue scale limits |Maximum parallel job pods allowed                                        |                                       |
| `scaledJob.concurrencyPolicy` | `Forbid`                        | Prevents more than one job processing the same message                  |
| `jobTargetRef.backoffLimit`   | `0`                             | Retries not needed ‚Äî let SQS redelivery handle failed jobs              |
|  `successfulJobsHistoryLimit` | `2`                             | Keeps last 2 completed jobs (adjust as needed for debugging/monitoring) |
| `failedJobsHistoryLimit`      | `2`                             | Keeps last 2 failed jobs (adjust as needed for debugging/monitoring)    |

Notes on Guidance
1) Set backoffLimit: 0 if your message is reprocessed by SQS ‚Äî this avoids Kubernetes retries that might conflict with message reappearance

2) Set successfulJobsHistoryLimit / failedJobsHistoryLimit based on debugging needs vs. resource cleanup

3) Use concurrencyPolicy: Forbid to avoid concurrent duplicate processing

4) Prefer the accurate scaling strategy for better control
```
apiVersion: keda.sh/v1alpha1
kind: ScaledJob
metadata:
  name: my-sqs-worker
spec:
  pollingInterval: 10
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 2
  maxReplicaCount: 10
  jobTargetRef:
    parallelism: 1
    backoffLimit: 0  # Let SQS handle retries instead of Kubernetes
  strategy:
    strategy: "accurate"
  triggers:
    - type: aws-sqs-queue
      metadata:
        queueURL: https://sqs.us-east-1.amazonaws.com/1234/my-queue
        awsRegion: us-east-1

```
Logging:
Use structured logs, ideally in JSON, for parsing in tools like Fluent Bit ‚Üí Splunk  / CloudWatch.
| Stage                       | Log Example / Message                                     |
| --------------------------- | --------------------------------------------------------- |
| Job Start                   | `"Starting KSJ job execution"`                            |
| SQS Polling                 | `"Polling SQS for message"`                               |
| Message Received            | `"Received message with ID: 123abc"`                      |
| Message Processing Started  | `"Started processing message: {...}"`                     |
| Visibility Timeout Extended | `"Extending visibility timeout by 30s"`                   |
| Processing Success          | `"Successfully processed message"`                        |
| Message Deleted             | `"Message deleted from SQS"`                              |
| Job Interrupted (SIGTERM)   | `"Received SIGTERM. Job interrupted."`                    |
| Exception / Failure         | `"Exception occurred during message processing: <error>"` |

Include contextual fields:

messageId, jobId, timestamp, status, queueName, durationMs, region

Metrics: use metrics for job health and throughput
| Metric                        | Use                                                                   |
| ----------------------------- | --------------------------------------------------------------------- |
| `job_start_total`             | Count of job starts                                                   |
| `job_success_total`           | Count of successful message processing                                |
| `job_failure_total`           | Count of failed jobs                                                  |
| `job_duration_seconds`        | Histogram of job processing time                                      |
| `visibility_extensions_total` | Number of visibility extensions (to catch anomalies or infinite jobs) |
