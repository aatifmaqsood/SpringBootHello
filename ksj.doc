Objective:
This document defines the reference architecture and behavioral standards for Kubernetes Scaling Jobs (KSJ) used for asynchronous message processing in our platform. It outlines how we prefer a KSJ to function, handles edge cases, and defines default configurations for KEDA, SQS, and job lifecycle.


Summary
1) A Kubernetes Scaling Job (KSJ) is designed to:
2) Dynamically scale based on queue depth using KEDA
3) Process messages from SQS
4) Ensure messages are reliably handled, retried if needed, and never lost silently
5) Provide observability through structured logs
6) Be resilient to failures and handle graceful shutdowns


updated summary:
This KSJ sample app is designed to:

1) Process a single message from SQS per job execution
2) Use KEDA to dynamically scale jobs based on SQS queue depth
3) Extend message visibility timeout dynamically during long-running processing
4) Handle SIGTERM signals gracefully to avoid silent message loss
5) Provide structured logs compatible with Fluent Bit → Splunk or other observability pipelines

Reference Workflow:
***
+-------------+ +------------+ +----------------+
| Developer/API | -----> | S3 | -----> | SQS |
+-------------+ +------------+ +----------------+
|
v
+---------------------------+
| KEDA ScaledObject |
| (SQS Trigger) |
+---------------------------+
|
v
+--------------------------+
| K8s ScaledJob (KSJ)|
+--------------------------+
|
+------------------------+-------------------+-------------------------+
| | |
Read SQS Msg Execute Business Logic Log + Retry Policy
| | |
On Success On Success On Failure
| | |
Delete SQS Msg Terminate Pod Let message timeout (remain in
queue)



    +------------------------------+
          |     Kubernetes Scaling Job   |
          |    (KSJ Worker Container)    |
          +------------------------------+
                            |
                            v
             +------------------------------+
             |  Read Message from SQS       |  ← Visibility Timeout Starts
             +------------------------------+
                            |
                            v
             +------------------------------+
             |  Start Background Thread     |
             |  to Extend Visibility        |
             +------------------------------+
                            |
                            v
             +------------------------------+
             |  Execute Job Logic           |
             |  (e.g. process order, image) |
             +------------------------------+
                            |
            / \                             \
           /   \                             \ SIGTERM received?
   Success     Failure                        → Set shutdown_event
     |             |                          → Do NOT delete message
     |             v
     |     +-----------------------------+
     |     |  Log Error, Let Timeout Expire
     |     +-----------------------------+
     |
     v
+-----------------------------+
|   Delete Message from SQS   |
+-----------------------------+

Implementation Guidelines
1. Message Visibility Timeout:
The sample KSJ worker uses Amazon SQS visibility timeout to ensure messages are not processed by more than one worker simultaneously. This section outlines how timeouts are configured and dynamically extended during job execution.
| Feature               | Recommendation                                                        |
| --------------------- | --------------------------------------------------------------------- |
| **Initial Timeout**   | 30–60 seconds (via `ReceiveMessage`)                                  |
| **Dynamic Extension** | Use if job duration is variable or long                               |
| **Maximum Timeout**   | Should match max expected processing time (e.g., 10 minutes)          |
| **On Failure**        | Don’t delete message – let it become visible again after timeout      |


Initial Timeout (30 seconds)

```VISIBILITY_TIMEOUT = int(os.getenv("INITIAL_VISIBILITY_TIMEOUT", 30))  # Default 30s

response = sqs.receive_message(
    QueueUrl=QUEUE_URL,
    MaxNumberOfMessages=1,
    WaitTimeSeconds=10,
    VisibilityTimeout=VISIBILITY_TIMEOUT  # <-- Initial visibility timeout
)
```

java sample code:
```
import software.amazon.awssdk.services.sqs.SqsClient;
import software.amazon.awssdk.services.sqs.model.ReceiveMessageRequest;
import software.amazon.awssdk.services.sqs.model.Message;
import software.amazon.awssdk.regions.Region;
import java.util.List;

public class SqsReceiver {

    private static final String QUEUE_URL = System.getenv("QUEUE_URL");
    private static final int INITIAL_VISIBILITY_TIMEOUT = Integer.parseInt(
        System.getenv().getOrDefault("INITIAL_VISIBILITY_TIMEOUT", "30")  // Default 30 seconds
    );

    public static void main(String[] args) {
        SqsClient sqsClient = SqsClient.builder()
                .region(Region.of("us-east-1")) // Set region appropriately
                .build();

        ReceiveMessageRequest receiveRequest = ReceiveMessageRequest.builder()
                .queueUrl(QUEUE_URL)
                .maxNumberOfMessages(1)
                .waitTimeSeconds(10)
                .visibilityTimeout(INITIAL_VISIBILITY_TIMEOUT)
                .build();

        List<Message> messages = sqsClient.receiveMessage(receiveRequest).messages();

        if (messages.isEmpty()) {
            System.out.println("No messages received.");
        } else {
            Message message = messages.get(0);
            System.out.println("Received message: " + message.body());
            // Process your message here
        }

        sqsClient.close();
    }
}
```


Recommendation:
Set the initial visibility timeout between 30–60 seconds — long enough for most short jobs, but short enough to allow retries if the job crashes quickly.

Dynamic Extension:
```
EXTEND_INTERVAL = int(os.getenv("VISIBILITY_EXTENSION_INTERVAL", 10))  # every 10s
EXTEND_BY = int(os.getenv("EXTEND_BY", 30))  # extend by 30s

def extend_visibility_timeout():
    while not shutdown_event.is_set():
        time.sleep(EXTEND_INTERVAL)
        if receipt_handle:
            try:
                logging.info(f"Extending visibility timeout by {EXTEND_BY}s")
                sqs.change_message_visibility(
                    QueueUrl=QUEUE_URL,
                    ReceiptHandle=receipt_handle,
                    VisibilityTimeout=EXTEND_BY
                )
            except Exception as e:
                logging.error(f"Failed to extend visibility timeout: {e}")
```
1*************
java sample code:
```
import jakarta.annotation.PreDestroy;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;
import software.amazon.awssdk.services.sqs.SqsClient;
import software.amazon.awssdk.services.sqs.model.ChangeMessageVisibilityRequest;

import java.util.concurrent.atomic.AtomicBoolean;

@Slf4j
@Component
public class VisibilityExtender {

    private final int EXTEND_INTERVAL = Integer.parseInt(System.getenv().getOrDefault("VISIBILITY_EXTENSION_INTERVAL", "10")); // seconds
    private final int EXTEND_BY = Integer.parseInt(System.getenv().getOrDefault("EXTEND_BY", "30")); // seconds

    private final SqsClient sqsClient;
    private final String queueUrl = System.getenv("QUEUE_URL");
    private final AtomicBoolean shuttingDown = new AtomicBoolean(false);
    private Thread extensionThread;

    public VisibilityExtender(SqsClient sqsClient) {
        this.sqsClient = sqsClient;
    }

    public void start(String receiptHandle) {
        extensionThread = new Thread(() -> {
            try {
                while (!shuttingDown.get()) {
                    Thread.sleep(EXTEND_INTERVAL * 1000L);

                    if (receiptHandle != null && !shuttingDown.get()) {
                        log.info("Extending visibility timeout by {}s", EXTEND_BY);

                        ChangeMessageVisibilityRequest req = ChangeMessageVisibilityRequest.builder()
                                .queueUrl(queueUrl)
                                .receiptHandle(receiptHandle)
                                .visibilityTimeout(EXTEND_BY)
                                .build();

                        try {
                            sqsClient.changeMessageVisibility(req);
                        } catch (Exception e) {
                            log.error("Failed to extend visibility timeout", e);
                        }
                    }
                }
            } catch (InterruptedException ex) {
                Thread.currentThread().interrupt();
                log.warn("Visibility extension thread interrupted");
            }
        });

        extensionThread.setDaemon(true);
        extensionThread.start();
    }

    @PreDestroy
    public void shutdown() {
        log.warn("Shutdown signal received. Stopping visibility extension.");
        shuttingDown.set(true);
        if (extensionThread != null && extensionThread.isAlive()) {
            extensionThread.interrupt();
        }
    }
}
```

Usage in a Message Processing Service
```
@Component
public class MessageProcessor {

    private final VisibilityExtender visibilityExtender;

    public MessageProcessor(VisibilityExtender visibilityExtender) {
        this.visibilityExtender = visibilityExtender;
    }

    public void processMessage(String messageBody, String receiptHandle) {
        visibilityExtender.start(receiptHandle);

        try {
            // Your message processing logic here
            // Simulate work
            Thread.sleep(45000); // 45s

            // After successful processing, delete the message
            // (not shown here — use SqsClient.deleteMessage)

        } catch (Exception e) {
            // log and let the message become visible again
        }
    }
}
```
1*********
A background thread runs every EXTEND_INTERVAL to extend the timeout while the job is still working.
Recommendation for Dynamic Extension:
We recommend dynamic visibility timeout only if:

1) Our job has variable processing times, and static timeouts would be inefficient

2) We want faster recovery from job crashes (e.g., via retry within 30s instead of waiting full max timeout)

3) We are not using FIFO queues with deduplication delay issues

4) If Our job always takes 60 seconds, we may skip dynamic extension and just set VisibilityTimeout=60.

On Failure – Do Not Delete Message
```
try:
    process_message(message_body)

    if shutdown_event.is_set():
        logging.warning("Job interrupted. Message will return to queue.")
    else:
        logging.info("Deleting message from queue")
        sqs.delete_message(
            QueueUrl=QUEUE_URL,
            ReceiptHandle=receipt_handle
        )

except Exception as e:
    logging.error(f"Job failed with exception: {e}")
    # Message is NOT deleted — becomes visible again after timeout
```



-------------------------------------------

Rationale Behind This Recommendation
The key idea behind dynamic visibility timeout is balancing between processing safety and retry latency:

1) Without it: A failed job may block the message for the entire static timeout (e.g., 600s), delaying retry or DLQ routing.

2) With it: We start with a shorter timeout (e.g., 30s), and extend only as needed while the job is actively working.

This:

A) Reduces time-to-retry on failure

B) Prevents duplicate delivery during long processing

C) Avoids overestimating timeout for all jobs (which wastes time on failure paths)

D) Keeps jobs simple — We only need to know the message is still in-flight, not how long it will take


When Not to Use It?
A) If Our job processing time is always known and consistent, just set a static VisibilityTimeout = max processing time.

B) If using FIFO queues where deduplication delay causes issues, prefer a consistent visibility timeout.



-----------------------------------------------








If the job crashes or is interrupted (SIGTERM), the message is left in SQS and reappears after timeout.



2) Read Message → Process Message →  Delete Message
following code snippet is just a sample app reference

 a. Read 1 Message from SQS
```
response = sqs.receive_message(
    QueueUrl=QUEUE_URL,
    MaxNumberOfMessages=1,  # 👈 Reads only one message per job
    WaitTimeSeconds=10,
    VisibilityTimeout=VISIBILITY_TIMEOUT
)
```

This is the message retrieval step.

MaxNumberOfMessages=1: Ensures one job handles one message.

VisibilityTimeout: Ensures the message becomes invisible to others while the job is processing it.

Note: This makes the job scalable, isolated, and avoids concurrent processing of the same message.

b. Process the Message
following code snippet is just a sample app reference
```
def process_message(message_body, receipt_handle):
    logging.info(f"Started processing message: {message_body}")

    total_processing_time = 60  # Total simulated job time in seconds
    elapsed = 0

    while elapsed < total_processing_time and not shutdown_event.is_set():
        time.sleep(EXTEND_INTERVAL)  # ⏱️ Sleep for configured interval
        elapsed += EXTEND_INTERVAL

        logging.info(f"Processed {elapsed}/{total_processing_time} seconds")

        try:
            logging.info(f"Extending visibility timeout by {EXTEND_BY}s")
            sqs.change_message_visibility(
                QueueUrl=QUEUE_URL,
                ReceiptHandle=receipt_handle,
                VisibilityTimeout=EXTEND_BY
            )
        except Exception as e:
            logging.error(f"Failed to extend visibility timeout: {e}")

    if shutdown_event.is_set():
        logging.warning("Processing was interrupted by shutdown.")
    else:
        logging.info("Finished processing message")

```

here we will be having our code logic.

| Config Var        | Usage in Function                                        |
| ----------------- | -------------------------------------------------------- |
| `EXTEND_INTERVAL` | Used as sleep duration in the loop — how often to extend |
| `EXTEND_BY`       | Duration to extend visibility on each API call           |

c. Delete the Message (Only on Success)
```
if shutdown_event.is_set():
    logging.warning("Job interrupted. Message will return to queue.")
else:
    logging.info("Deleting message from queue")
    sqs.delete_message(
        QueueUrl=QUEUE_URL,
        ReceiptHandle=receipt_handle
    )
```


If processing completed successfully and job was not interrupted, the message is explicitly deleted from the queue.

This prevents it from being reprocessed.

Note: Only delete after successful processing.



----------------------------------------------------------
2. Read → Process → Delete: Message Lifecycle 
This section highlights the standard pattern used in Kubernetes Scaling Jobs (KSJs) for processing AWS SQS messages reliably.
lifecycle ensures:

Each job processes only one message

Messages are not lost or duplicated

Dynamic extension ensures efficient processing

Failed or interrupted jobs will not delete the message

a) Read Message:
```
ReceiveMessageRequest request = ReceiveMessageRequest.builder()
        .queueUrl(QUEUE_URL)
        .maxNumberOfMessages(1)
        .visibilityTimeout(INITIAL_VISIBILITY_TIMEOUT)  // e.g., 30s
        .waitTimeSeconds(10) // long polling
        .build();

List<Message> messages = sqs.receiveMessage(request).messages();

if (messages.isEmpty()) {
    System.out.println("No messages received.");
    return;
}
Message message = messages.get(0);
String receiptHandle = message.receiptHandle();
System.out.println("Received message: " + message.body());
```

Explanation:
maxNumberOfMessages = 1 ensures one message per job, making job processing isolated and scalable.

visibilityTimeout = 30s ensures the message is invisible to others during processing.

If the message is not deleted, it becomes visible again for retry.

b) Process Message with Dynamic Timeout Extension
```
AtomicBoolean shutdown = new AtomicBoolean(false);
int totalProcessingTime = 60;
int elapsed = 0;

Thread extender = new Thread(() -> {
    while (!shutdown.get()) {
        try {
            Thread.sleep(EXTEND_INTERVAL * 1000);  // e.g., every 10s
            System.out.println("Extending visibility by " + EXTEND_BY + " seconds");
            sqs.changeMessageVisibility(ChangeMessageVisibilityRequest.builder()
                    .queueUrl(QUEUE_URL)
                    .receiptHandle(receiptHandle)
                    .visibilityTimeout(EXTEND_BY)  // extend by 30s
                    .build());
        } catch (Exception e) {
            System.err.println("Failed to extend visibility: " + e.getMessage());
        }
    }
});
extender.start();

// Simulated processing
while (elapsed < totalProcessingTime && !shutdown.get()) {
    Thread.sleep(EXTEND_INTERVAL * 1000); // simulate work
    elapsed += EXTEND_INTERVAL;
    System.out.println("Processing... " + elapsed + "/" + totalProcessingTime);
}

```
Dynamic Timeout Parameters
| Parameter                    | Purpose                                          |
| ---------------------------- | ------------------------------------------------ |
| `INITIAL_VISIBILITY_TIMEOUT` | How long to hide message initially (e.g., 30s)   |
| `EXTEND_INTERVAL`            | How often to extend visibility (e.g., every 10s) |
| `EXTEND_BY`                  | By how much to extend each time (e.g., 30s)      |

Why Dynamic Extension?

3**************************
Scenario-Based Examples:


| Scenario                                                                 | Expected Behavior                                                                                          |
| ------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------- |
| **Job takes 90s, initial timeout is 30s**                             | Visibility is extended every 10s → message never becomes visible prematurely.                              |
| **Pod crashes at 20s with 30s timeout**                               | Message reappears after \~30s for retry → fast recovery. No manual retry logic needed.                     |
| **Job takes between 10s–120s depending on payload**                   | Short timeout + dynamic extension = fast retry on crash and no over-blocking on short messages.            |
| **Static timeout of 600s for all jobs, even if many complete in 20s** | Wastes queue time → dynamic extension reduces queue wait time, making fast jobs reattemptable sooner.      |
| **SIGTERM received during processing at 45s of a 60s task**            | `shutdown` flag avoids deleting the message → it becomes visible again safely after last extended timeout. |
| **Downstream system is slow (e.g., payment API delay)**                | Visibility extension keeps message invisible for longer → avoids early retry and duplicate API calls.      |

3****************************
c) Delete Message After Successful Processing
```
if (!shutdown.get()) {
    System.out.println("Deleting message from SQS...");
    sqs.deleteMessage(DeleteMessageRequest.builder()
            .queueUrl(QUEUE_URL)
            .receiptHandle(receiptHandle)
            .build());
} else {
    System.out.println("Processing interrupted. Message will return to the queue.");
}
shutdown.set(true);
extender.join();

```
Key Rule:
a) Only delete if job completes successfully and was not interrupted.

b) If the job fails or receives a SIGTERM, the message will reappear after timeout for retry or DLQ handling.





-------------------------------------------------------------







3) Handling SIGTERM:
When Kubernetes terminates a pod (e.g., scaling down or job completion), it sends a SIGTERM signal to give the container a chance to shut down cleanly before forcibly killing it with SIGKILL.

Without handling SIGTERM:

> our worker might exit mid-processing

> The message might get lost or never reappear (if deleted too early)

Every long-running loop (like the message processor or visibility extender) checks shutdown_event.is_set():

```
while elapsed < total_processing_time and not shutdown_event.is_set():
```
As soon as SIGTERM is received, shutdown_event is set
```
while not shutdown_event.is_set():
```
All loops will gracefully break out, log exit, and stop processing

Our Logic should integrates graceful SIGTERM handling correctly:

> Catches the signal

> Signals all threads to stop

> Skips deleting the message on early termination



------------------------------------------------------

3) Handling SIGTERM & Graceful Shutdown
In Kubernetes, when a pod is terminated (due to scale-down, job completion, or eviction), the container receives a SIGTERM signal. This is the opportunity for the container to gracefully shut down, clean up resources, and avoid message loss.

Why it matters
Without handling SIGTERM:

The job may be interrupted mid-processing

The message might be partially processed and then lost or redelivered unexpectedly

The worker might delete the message too early, even if it didn’t finish the job.

Step-by-Step Workflow:
> A shared flag (AtomicBoolean shutdown) is used across threads.

> When Kubernetes sends a SIGTERM, Spring Boot triggers @PreDestroy.

> The flag is set to true.

> All long-running loops check the flag and exit cleanly when shutdown begins.

shutdown handler:
```
import javax.annotation.PreDestroy;
import java.util.concurrent.atomic.AtomicBoolean;
import org.springframework.stereotype.Component;

@Component
public class GracefulShutdownHandler {

    private final AtomicBoolean shutdown = new AtomicBoolean(false);

    @PreDestroy
    public void onShutdown() {
        shutdown.set(true);
        System.out.println("[SHUTDOWN] SIGTERM received. Stopping gracefully...");
        // Add any cleanup code here: close DBs, flush logs, etc.
    }

    public boolean isShuttingDown() {
        return shutdown.get();
    }
}
```

with our processing loop:

```
@Autowired
private GracefulShutdownHandler shutdownHandler;

public void processMessage(String body, String receiptHandle) {
    int totalProcessingTime = 60;
    int elapsed = 0;

    while (elapsed < totalProcessingTime && !shutdownHandler.isShuttingDown()) {
        try {
            Thread.sleep(EXTEND_INTERVAL * 1000);  // Simulate work
            elapsed += EXTEND_INTERVAL;
            System.out.println("Processing... " + elapsed + "/" + totalProcessingTime + " seconds");

            // Extend visibility timeout here
            sqs.changeMessageVisibility(ChangeMessageVisibilityRequest.builder()
                    .queueUrl(QUEUE_URL)
                    .receiptHandle(receiptHandle)
                    .visibilityTimeout(EXTEND_BY)
                    .build());

        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            System.err.println("Interrupted while processing");
            break;
        }
    }

    if (shutdownHandler.isShuttingDown()) {
        System.out.println("Graceful shutdown triggered. Skipping message deletion.");
    } else {
        System.out.println("Finished. Deleting message.");
        sqs.deleteMessage(DeleteMessageRequest.builder()
                .queueUrl(QUEUE_URL)
                .receiptHandle(receiptHandle)
                .build());
    }
}
```
Key Points:
| Element                       | Description                                                     |
| ----------------------------- | --------------------------------------------------------------- |
| `@PreDestroy`                 | Called automatically on SIGTERM in Spring Boot                  |
| `AtomicBoolean` shutdown flag | Shared across worker & background threads                       |
| Graceful loop check           | Every loop (`while`) checks `!shutdownHandler.isShuttingDown()` |
| Skips deletion                | Ensures we **don’t delete** the message if job was interrupted  |


PreStop Hook Snippet for Alerts
What is it?
The preStop hook gives the container a final notification before shutdown. This allows us to:

> Log shutdown intent clearly (which can be picked up by Splunk alerts).

> Set flags or drain messages before SIGKILL.

kuberentes yaml sample:

```
lifecycle:
  preStop:
    exec:
      command: ["/bin/sh", "-c", "echo 'Shutting down pod gracefully' >> /var/log/app/shutdown.log"]
```

How this helps app teams:

> Allows deterministic cleanup before termination.

> Logs from preStop can be monitored in Splunk for alerting on abnormal shutdowns.

> App teams get visibility and time to gracefully drain workloads.

terminationGracePeriodSeconds:
kubernetes yaml sample:
```
spec:
  terminationGracePeriodSeconds: 60
```
Why 60 seconds?

> Default is 30s, which might be insufficient sometimes for message processing or cleanup.

> 60s allows enough time for:

    >Message completion

    >Flushing logs

    >Resource cleanup

How this fits into workflow:

> Kubernetes sends SIGTERM.

> App receives signal and starts shutdown logic.

> App completes in-flight tasks or logs intent.

> If it exceeds terminationGracePeriodSeconds, Kubernetes forcefully sends SIGKILL.

We can configure this per deployment via Helm chart values:
helm sample yaml
```
terminationGracePeriodSeconds: {{ .Values.app.terminationGracePeriodSeconds | default 60 }}
```

Java / Spring Boot Example
In Java, particularly with Spring Boot, graceful shutdown is typically done using a @PreDestroy method or a shutdown hook.

Spring Boot SIGTERM Handling Example

```
import org.springframework.stereotype.Component;
import javax.annotation.PreDestroy;

@Component
public class GracefulShutdownManager {

    private volatile boolean shuttingDown = false;

    public boolean isShuttingDown() {
        return shuttingDown;
    }

    @PreDestroy
    public void onShutdown() {
        shuttingDown = true;
        System.out.println("SIGTERM received. Initiating graceful shutdown...");
        // Example: stop consuming new messages, finish current work
        // Avoid deleting messages from queue until processing completes
    }
}
```

Thread-safe Usage in Worker Loops
```
while (!shutdownManager.isShuttingDown()) {
    // Fetch message
    // Process safely
}
System.out.println("Graceful exit completed.");
```
Summary of Recommendations

| Item                            | Recommendation                                                | Benefit                      |
| ------------------------------- | ------------------------------------------------------------- | ---------------------------- |
| `preStop` hook                  | Log shutdown events                                           | Alerts, observability        |
| `terminationGracePeriodSeconds` | Set to 60 seconds                                             | Gives time for cleanup       |
| Signal handling                 | Use `@PreDestroy` or `Runtime.getRuntime().addShutdownHook()` | Safe termination             |
| Logging                         | Ensure shutdown logs are visible                              | Helps in debugging, alerting |
| Queue behavior                  | Avoid deleting message on shutdown                            | Prevent message loss         |




----------------------------------------------------------------------










4)Logging 
All logs must be written to stdout

Use structured log format:

```
2025-06-23T10:00:00Z | INFO | Processing message: {messageId: abc123}
2025-06-23T10:00:15Z | WARN | SIGTERM received, exiting
2025-06-23T10:00:20Z | ERROR | Job failed with: TimeoutError
```
Required fields:

message_id, job_id, retry_count, duration, status, error



-----------------------------------------------------------
4) Observability & Logging
Logging Best Practices
All logs must be written to stdout in a structured log format for compatibility with log aggregation tools like Splunk, CloudWatch, or ELK.

Recommended pattern:
<timestamp> | <level> | <message> | <contextual fields>

Example:
2025-06-23T10:00:00Z | INFO  | Started processing message            | message_id=abc123 job_id=job789 retry_count=1
2025-06-23T10:00:05Z | INFO  | Extending message visibility by 30s  | message_id=abc123 job_id=job789
2025-06-23T10:00:15Z | WARN  | SIGTERM received, shutting down      | job_id=job789
2025-06-23T10:00:20Z | ERROR | Job failed                           | message_id=abc123 job_id=job789 error=TimeoutException duration=12000ms

Required Fields in Logs
Make sure to include these fields in relevant log statements:

> message_id

> job_id

> retry_count

> duration (in milliseconds)

> status

> error (if applicable)

sample log snippets:
```
logger.info("Started processing message", kv("message_id", msgId), kv("job_id", jobId), kv("retry_count", retryCount));
logger.info("Extending message visibility by 30 seconds", kv("message_id", msgId), kv("job_id", jobId));
logger.warn("Shutdown signal received, processing will stop gracefully", kv("job_id", jobId));
logger.error("Job failed", kv("message_id", msgId), kv("job_id", jobId), kv("error", exception), kv("duration", duration));
```
Use a structured logger like logstash-logback-encoder, SLF4J with MDC, or similar for consistent key-value output.

Key Metrics to Track
Metrics help us understand performance and reliability trends over time.

| Metric                                        | Description                            |
| --------------------------------------------- | -------------------------------------- |
| **`processing_duration_ms`**                  | Time taken to process a message        |
| **`retry_count`**                             | Number of times a job was retried      |
| **`message_visibility_extensions`**           | Count of visibility extension requests |
| **`job_success_count` / `job_failure_count`** | Success/failure metrics                |
| **`sigterm_received_count`**                  | Tracks shutdown signals received       |

example metric Logging:
```
long duration = System.currentTimeMillis() - startTime;
logger.info("Finished processing", kv("message_id", msgId), kv("job_id", jobId), kv("duration", duration), kv("status", "success"));
```

Dashboards and Monitoring Recommendations
To ensure complete visibility into job performance and pod stability:

> OOMKilled Dashboard
Monitor memory usage trends and track containers terminated with OOMKilled status.
Recommended: Use Prometheus + Grafana or Cloud-native dashboards like GKE Workload Details.

> Job Failure Dashboard
Track:

  > Job error types (Timeout, Exception, etc.)

  > Frequency of retries

  > Duration before failure

  > Message IDs of failed jobs


---------------------------------------------------------------------


5) Keda Configurations:
This configuration defines how the KSJ sample app integrates with KEDA ScaledJob for scaling behavior.
These values are reference defaults for this sample pattern. Actual values should be adjusted based on your job’s behavior, retry policy, and cost/resource tradeoffs.

| Setting                       | Recommended Default             | Description                                                             |
| ----------------------------- | ------------------------------- | ------------------------------------------------------------------------|
| `pollingInterval`             | `30s`                           | Frequency at which KEDA checks SQS for pending messages                 |
| `minReplicaCount`             | `0`                             |Zero idle pods; scale from zero                                          |
| `maxReplicaCount`             | Set to match queue scale limits |Maximum parallel job pods allowed                                        |                                       |
| `jobTargetRef.backoffLimit`   | `0`                             | Retries not needed — let SQS redelivery handle failed jobs              |
|  `successfulJobsHistoryLimit` | `2`                             | Keeps last 2 completed jobs (adjust as needed for debugging/monitoring) |
| `failedJobsHistoryLimit`      | `2`                             | Keeps last 2 failed jobs (adjust as needed for debugging/monitoring)    |

Notes on Guidance
1) Set backoffLimit: 0 if your message is reprocessed by SQS — this avoids Kubernetes retries that might conflict with message reappearance

2) Set successfulJobsHistoryLimit / failedJobsHistoryLimit based on debugging needs vs. resource cleanup

3) Use concurrencyPolicy: Forbid to avoid concurrent duplicate processing

4) Prefer the accurate scaling strategy for better control
2*******
pollingInterval: If we expect lower traffic or cost optimization is a higher priority, increasing the interval to 30 seconds is valid and reduces unnecessary polling.
However, we may experience a slight increase in message processing latency, which should be acceptable in non-time-critical workflows.
For Dev/Test we may consider 10 seconds for faster feedback but for Prod 30 seconds (or more) for efficiency is acceptable.

scalingStrategy: The scalingStrategy: accurate setting is a Kubernetes HPA/KEDA-style directive (depending on implementation) that prioritizes precision over speed when scaling applications. It calculates required replicas based on actual metrics (like CPU, memory, queue length, etc.), ensuring the pod count is finely tuned to demand.

scalingStrategy types and when to use what:
| Strategy                | When to Use                                         | Characteristics                                     |
| ----------------------- | --------------------------------------------------- | --------------------------------------------------- |
| `accurate`              | Default for cost efficiency and stability           | Conservative scaling, based on actual metrics       |
| `default`               | Quick setup when metrics or tuning aren't available | May not be optimal for spiky traffic                |
| `aggressive`            | For latency-critical or bursty workloads            | Scales up quickly, might over-provision temporarily |
| `custom` (if supported) | When specific heuristics are needed                 | Fully customizable scaling behavior                 |

Recommendation
Stick with accurate for:
    > Production workloads
    > Queued systems (SQS, Kafka, etc.)
    > Jobs where cost-efficiency and stable throughput matter
We’ve chosen scalingStrategy: accurate as the default because:

> Predictable Scaling Behavior: Prevents over-scaling or under-scaling, which is especially critical in cost-sensitive or resource-constrained environments.

> Stabilizes workloads: Reduces flapping (constant scaling up/down), which can happen with more aggressive strategies.

> Metric-aware decisions: Accurate strategy uses real-time metrics (e.g., queue depth, CPU), ensuring pods scale only when truly necessary.

> Ideal for background jobs / workers: These often have variable load — accurate helps maintain just the right number of workers.

restartPolicy: Never
> Ensures failed pods are not restarted by Kubernetes.

> Once the pod finishes (successfully or with failure), it terminates permanently.

> Aligns with event-driven jobs: one trigger = one job = one execution.

backoffLimit: 0
> Ensures no retries are attempted by the Job controller upon failure.

> The Job is marked as Failed immediately after one unsuccessful attempt.

When to Use preventJobRestart: true
Use this setting when:

    > Retrying is unsafe or undesirable (e.g., payment processing, downstream irreversible actions).

    > You want fast failure visibility for debugging/logging.

    > Errors are known to be non-transient (e.g., malformed input, missing config).

    > You have external logic (like message requeue) to handle retries safely.

Example use cases:
    > Processing non-idempotent messages from a queue.

    > One-off executions where a single failure should trigger alerts or reprocessing externally.

    > Validation-only jobs.

```
apiVersion: keda.sh/v1alpha1
kind: ScaledJob
metadata:
  name: message-processor
spec:
  pollingInterval: 30                            # Check every 30s for new work
  successfulJobsHistoryLimit: 2                  # Retain history of last 2 successful jobs
  scalingStrategy: accurate                      # Avoid over/under-scaling
  jobTargetRef:
    parallelism: 1                               # The maximum number of pods running in parallel for a Job.
    completions: 1                               # The total number of pods that must succeed before the Job ends.
    backoffLimit: 0                              # Fail fast, no retries
    template:
      spec:
        restartPolicy: Never                     # Do not restart failed pods
        containers:
          - name: processor
            image: my-registry/worker:latest
            env:
              - name: MESSAGE_QUEUE
                value: my-queue
  triggers:
    - type: aws-sqs
      metadata:
        queueURL: https://sqs.us-east-1.amazonaws.com/1234567890/my-queue
        queueLength: "5"                           #For every 5 messages in the queue, KEDA will launch 1 job (pod).


```
2**************************
Logging:
Use structured logs, ideally in JSON, for parsing in tools like Fluent Bit → Splunk  / CloudWatch.
| Stage                       | Log Example / Message                                     |
| --------------------------- | --------------------------------------------------------- |
| Job Start                   | `"Starting KSJ job execution"`                            |
| SQS Polling                 | `"Polling SQS for message"`                               |
| Message Received            | `"Received message with ID: 123abc"`                      |
| Message Processing Started  | `"Started processing message: {...}"`                     |
| Visibility Timeout Extended | `"Extending visibility timeout by 30s"`                   |
| Processing Success          | `"Successfully processed message"`                        |
| Message Deleted             | `"Message deleted from SQS"`                              |
| Job Interrupted (SIGTERM)   | `"Received SIGTERM. Job interrupted."`                    |
| Exception / Failure         | `"Exception occurred during message processing: <error>"` |

Include contextual fields:

messageId, jobId, timestamp, status, queueName, durationMs, region

Metrics: use metrics for job health and throughput
| Metric                        | Use                                                                   |
| ----------------------------- | --------------------------------------------------------------------- |
| `job_start_total`             | Count of job starts                                                   |
| `job_success_total`           | Count of successful message processing                                |
| `job_failure_total`           | Count of failed jobs                                                  |
| `job_duration_seconds`        | Histogram of job processing time                                      |
| `visibility_extensions_total` | Number of visibility extensions (to catch anomalies or infinite jobs) |
