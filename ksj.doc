Objective:
This document defines the reference architecture and behavioral standards for Kubernetes Scaling Jobs (KSJ) used for asynchronous message processing in our platform. It outlines how we prefer a KSJ to function, handles edge cases, and defines default configurations for KEDA, SQS, and job lifecycle.


Summary
1) A Kubernetes Scaling Job (KSJ) is designed to:
2) Dynamically scale based on queue depth using KEDA
3) Process messages from SQS
4) Ensure messages are reliably handled, retried if needed, and never lost silently
5) Provide observability through structured logs
6) Be resilient to failures and handle graceful shutdowns


updated summary:
This KSJ sample app is designed to:

1) Process a single message from SQS per job execution
2) Use KEDA to dynamically scale jobs based on SQS queue depth
3) Extend message visibility timeout dynamically during long-running processing
4) Handle SIGTERM signals gracefully to avoid silent message loss
5) Provide structured logs compatible with Fluent Bit ‚Üí Splunk or other observability pipelines

Reference Workflow:
***
+-------------+ +------------+ +----------------+
| Developer/API | -----> | S3 | -----> | SQS |
+-------------+ +------------+ +----------------+
|
v
+---------------------------+
| KEDA ScaledObject |
| (SQS Trigger) |
+---------------------------+
|
v
+--------------------------+
| K8s ScaledJob (KSJ)|
+--------------------------+
|
+------------------------+-------------------+-------------------------+
| | |
Read SQS Msg Execute Business Logic Log + Retry Policy
| | |
On Success On Success On Failure
| | |
Delete SQS Msg Terminate Pod Let message timeout (remain in
queue)



    +------------------------------+
          |     Kubernetes Scaling Job   |
          |    (KSJ Worker Container)    |
          +------------------------------+
                            |
                            v
             +------------------------------+
             |  Read Message from SQS       |  ‚Üê Visibility Timeout Starts
             +------------------------------+
                            |
                            v
             +------------------------------+
             |  Start Background Thread     |
             |  to Extend Visibility        |
             +------------------------------+
                            |
                            v
             +------------------------------+
             |  Execute Job Logic           |
             |  (e.g. process order, image) |
             +------------------------------+
                            |
            / \                             \
           /   \                             \ SIGTERM received?
   Success     Failure                        ‚Üí Set shutdown_event
     |             |                          ‚Üí Do NOT delete message
     |             v
     |     +-----------------------------+
     |     |  Log Error, Let Timeout Expire
     |     +-----------------------------+
     |
     v
+-----------------------------+
|   Delete Message from SQS   |
+-----------------------------+

Implementation Guidelines
1. Message Visibility Timeout:
The sample KSJ worker uses Amazon SQS visibility timeout to ensure messages are not processed by more than one worker simultaneously. This section outlines how timeouts are configured and dynamically extended during job execution.
| Feature               | Recommendation                                                        |
| --------------------- | --------------------------------------------------------------------- |
| **Initial Timeout**   | 30‚Äì60 seconds (via `ReceiveMessage`)                                  |
| **Dynamic Extension** | Use if job duration is variable or long                               |
| **Maximum Timeout**   | Should match max expected processing time (e.g., 10 minutes)          |
| **On Failure**        | Don‚Äôt delete message ‚Äì let it become visible again after timeout      |


Initial Timeout (30 seconds)

```VISIBILITY_TIMEOUT = int(os.getenv("INITIAL_VISIBILITY_TIMEOUT", 30))  # Default 30s

response = sqs.receive_message(
    QueueUrl=QUEUE_URL,
    MaxNumberOfMessages=1,
    WaitTimeSeconds=10,
    VisibilityTimeout=VISIBILITY_TIMEOUT  # <-- Initial visibility timeout
)
```
Recommendation:
Set the initial visibility timeout between 30‚Äì60 seconds ‚Äî long enough for most short jobs, but short enough to allow retries if the job crashes quickly.

Dynamic Extension:
```
EXTEND_INTERVAL = int(os.getenv("VISIBILITY_EXTENSION_INTERVAL", 10))  # every 10s
EXTEND_BY = int(os.getenv("EXTEND_BY", 30))  # extend by 30s

def extend_visibility_timeout():
    while not shutdown_event.is_set():
        time.sleep(EXTEND_INTERVAL)
        if receipt_handle:
            try:
                logging.info(f"Extending visibility timeout by {EXTEND_BY}s")
                sqs.change_message_visibility(
                    QueueUrl=QUEUE_URL,
                    ReceiptHandle=receipt_handle,
                    VisibilityTimeout=EXTEND_BY
                )
            except Exception as e:
                logging.error(f"Failed to extend visibility timeout: {e}")
```
A background thread runs every EXTEND_INTERVAL to extend the timeout while the job is still working.
Recommendation for Dynamic Extension:
We recommend dynamic visibility timeout only if:

1) Our job has variable processing times, and static timeouts would be inefficient

2) We want faster recovery from job crashes (e.g., via retry within 30s instead of waiting full max timeout)

3) We are not using FIFO queues with deduplication delay issues

4) If Our job always takes 60 seconds, we may skip dynamic extension and just set VisibilityTimeout=60.

On Failure ‚Äì Do Not Delete Message
```
try:
    process_message(message_body)

    if shutdown_event.is_set():
        logging.warning("Job interrupted. Message will return to queue.")
    else:
        logging.info("Deleting message from queue")
        sqs.delete_message(
            QueueUrl=QUEUE_URL,
            ReceiptHandle=receipt_handle
        )

except Exception as e:
    logging.error(f"Job failed with exception: {e}")
    # Message is NOT deleted ‚Äî becomes visible again after timeout
```



-------------------------------------------

Rationale Behind This Recommendation
The key idea behind dynamic visibility timeout is balancing between processing safety and retry latency:

1) Without it: A failed job may block the message for the entire static timeout (e.g., 600s), delaying retry or DLQ routing.

2) With it: We start with a shorter timeout (e.g., 30s), and extend only as needed while the job is actively working.

This:

A) Reduces time-to-retry on failure

B) Prevents duplicate delivery during long processing

C) Avoids overestimating timeout for all jobs (which wastes time on failure paths)

D) Keeps jobs simple ‚Äî We only need to know the message is still in-flight, not how long it will take


When Not to Use It?
A) If Our job processing time is always known and consistent, just set a static VisibilityTimeout = max processing time.

B) If using FIFO queues where deduplication delay causes issues, prefer a consistent visibility timeout.



-----------------------------------------------








If the job crashes or is interrupted (SIGTERM), the message is left in SQS and reappears after timeout.



2) Read Message ‚Üí Process Message ‚Üí  Delete Message
following code snippet is just a sample app reference

 a. Read 1 Message from SQS
```
response = sqs.receive_message(
    QueueUrl=QUEUE_URL,
    MaxNumberOfMessages=1,  # üëà Reads only one message per job
    WaitTimeSeconds=10,
    VisibilityTimeout=VISIBILITY_TIMEOUT
)
```

This is the message retrieval step.

MaxNumberOfMessages=1: Ensures one job handles one message.

VisibilityTimeout: Ensures the message becomes invisible to others while the job is processing it.

Note: This makes the job scalable, isolated, and avoids concurrent processing of the same message.

b. Process the Message
following code snippet is just a sample app reference
```
def process_message(message_body, receipt_handle):
    logging.info(f"Started processing message: {message_body}")

    total_processing_time = 60  # Total simulated job time in seconds
    elapsed = 0

    while elapsed < total_processing_time and not shutdown_event.is_set():
        time.sleep(EXTEND_INTERVAL)  # ‚è±Ô∏è Sleep for configured interval
        elapsed += EXTEND_INTERVAL

        logging.info(f"Processed {elapsed}/{total_processing_time} seconds")

        try:
            logging.info(f"Extending visibility timeout by {EXTEND_BY}s")
            sqs.change_message_visibility(
                QueueUrl=QUEUE_URL,
                ReceiptHandle=receipt_handle,
                VisibilityTimeout=EXTEND_BY
            )
        except Exception as e:
            logging.error(f"Failed to extend visibility timeout: {e}")

    if shutdown_event.is_set():
        logging.warning("Processing was interrupted by shutdown.")
    else:
        logging.info("Finished processing message")

```

here we will be having our code logic.

| Config Var        | Usage in Function                                        |
| ----------------- | -------------------------------------------------------- |
| `EXTEND_INTERVAL` | Used as sleep duration in the loop ‚Äî how often to extend |
| `EXTEND_BY`       | Duration to extend visibility on each API call           |

c. Delete the Message (Only on Success)
```
if shutdown_event.is_set():
    logging.warning("Job interrupted. Message will return to queue.")
else:
    logging.info("Deleting message from queue")
    sqs.delete_message(
        QueueUrl=QUEUE_URL,
        ReceiptHandle=receipt_handle
    )
```


If processing completed successfully and job was not interrupted, the message is explicitly deleted from the queue.

This prevents it from being reprocessed.

Note: Only delete after successful processing.



----------------------------------------------------------
2. Read ‚Üí Process ‚Üí Delete: Message Lifecycle 
This section highlights the standard pattern used in Kubernetes Scaling Jobs (KSJs) for processing AWS SQS messages reliably.
lifecycle ensures:

Each job processes only one message

Messages are not lost or duplicated

Dynamic extension ensures efficient processing

Failed or interrupted jobs will not delete the message

a) Read Message:
```
ReceiveMessageRequest request = ReceiveMessageRequest.builder()
        .queueUrl(QUEUE_URL)
        .maxNumberOfMessages(1)
        .visibilityTimeout(INITIAL_VISIBILITY_TIMEOUT)  // e.g., 30s
        .waitTimeSeconds(10) // long polling
        .build();

List<Message> messages = sqs.receiveMessage(request).messages();

if (messages.isEmpty()) {
    System.out.println("No messages received.");
    return;
}
Message message = messages.get(0);
String receiptHandle = message.receiptHandle();
System.out.println("Received message: " + message.body());
```

Explanation:
maxNumberOfMessages = 1 ensures one message per job, making job processing isolated and scalable.

visibilityTimeout = 30s ensures the message is invisible to others during processing.

If the message is not deleted, it becomes visible again for retry.

b) Process Message with Dynamic Timeout Extension
```
AtomicBoolean shutdown = new AtomicBoolean(false);
int totalProcessingTime = 60;
int elapsed = 0;

Thread extender = new Thread(() -> {
    while (!shutdown.get()) {
        try {
            Thread.sleep(EXTEND_INTERVAL * 1000);  // e.g., every 10s
            System.out.println("Extending visibility by " + EXTEND_BY + " seconds");
            sqs.changeMessageVisibility(ChangeMessageVisibilityRequest.builder()
                    .queueUrl(QUEUE_URL)
                    .receiptHandle(receiptHandle)
                    .visibilityTimeout(EXTEND_BY)  // extend by 30s
                    .build());
        } catch (Exception e) {
            System.err.println("Failed to extend visibility: " + e.getMessage());
        }
    }
});
extender.start();

// Simulated processing
while (elapsed < totalProcessingTime && !shutdown.get()) {
    Thread.sleep(EXTEND_INTERVAL * 1000); // simulate work
    elapsed += EXTEND_INTERVAL;
    System.out.println("Processing... " + elapsed + "/" + totalProcessingTime);
}

```
Dynamic Timeout Parameters
| Parameter                    | Purpose                                          |
| ---------------------------- | ------------------------------------------------ |
| `INITIAL_VISIBILITY_TIMEOUT` | How long to hide message initially (e.g., 30s)   |
| `EXTEND_INTERVAL`            | How often to extend visibility (e.g., every 10s) |
| `EXTEND_BY`                  | By how much to extend each time (e.g., 30s)      |

Why Dynamic Extension?
Scenario-Based Examples:
| Scenario                                      | Why Dynamic Timeout Helps                            |
| --------------------------------------------- | ---------------------------------------------------- |
| Processing time varies between 10s and 2 mins | Set short initial timeout and extend dynamically     |
| Pod crashes at 20s during a 90s task          | Message reappears in \~30s instead of waiting 90s    |
| Some messages are fast (10s), some slow (60s) | Don‚Äôt block the queue with unnecessary long timeouts |

c) Delete Message After Successful Processing
```
if (!shutdown.get()) {
    System.out.println("Deleting message from SQS...");
    sqs.deleteMessage(DeleteMessageRequest.builder()
            .queueUrl(QUEUE_URL)
            .receiptHandle(receiptHandle)
            .build());
} else {
    System.out.println("Processing interrupted. Message will return to the queue.");
}
shutdown.set(true);
extender.join();

```
Key Rule:
a) Only delete if job completes successfully and was not interrupted.

b) If the job fails or receives a SIGTERM, the message will reappear after timeout for retry or DLQ handling.



-------------------------------------------------------------







3) Handling SIGTERM:
When Kubernetes terminates a pod (e.g., scaling down or job completion), it sends a SIGTERM signal to give the container a chance to shut down cleanly before forcibly killing it with SIGKILL.

Without handling SIGTERM:

> our worker might exit mid-processing

> The message might get lost or never reappear (if deleted too early)

Every long-running loop (like the message processor or visibility extender) checks shutdown_event.is_set():

```
while elapsed < total_processing_time and not shutdown_event.is_set():
```
As soon as SIGTERM is received, shutdown_event is set
```
while not shutdown_event.is_set():
```
All loops will gracefully break out, log exit, and stop processing

Our Logic should integrates graceful SIGTERM handling correctly:

> Catches the signal

> Signals all threads to stop

> Skips deleting the message on early termination



------------------------------------------------------

3) Handling SIGTERM & Graceful Shutdown
In Kubernetes, when a pod is terminated (due to scale-down, job completion, or eviction), the container receives a SIGTERM signal. This is the opportunity for the container to gracefully shut down, clean up resources, and avoid message loss.

Why it matters
Without handling SIGTERM:

The job may be interrupted mid-processing

The message might be partially processed and then lost or redelivered unexpectedly

The worker might delete the message too early, even if it didn‚Äôt finish the job.

Step-by-Step Workflow:
> A shared flag (AtomicBoolean shutdown) is used across threads.

> When Kubernetes sends a SIGTERM, Spring Boot triggers @PreDestroy.

> The flag is set to true.

> All long-running loops check the flag and exit cleanly when shutdown begins.

shutdown handler:
```
import javax.annotation.PreDestroy;
import java.util.concurrent.atomic.AtomicBoolean;
import org.springframework.stereotype.Component;

@Component
public class GracefulShutdownHandler {

    private final AtomicBoolean shutdown = new AtomicBoolean(false);

    @PreDestroy
    public void onShutdown() {
        shutdown.set(true);
        System.out.println("[SHUTDOWN] SIGTERM received. Stopping gracefully...");
        // Add any cleanup code here: close DBs, flush logs, etc.
    }

    public boolean isShuttingDown() {
        return shutdown.get();
    }
}
```

with our processing loop:

```
@Autowired
private GracefulShutdownHandler shutdownHandler;

public void processMessage(String body, String receiptHandle) {
    int totalProcessingTime = 60;
    int elapsed = 0;

    while (elapsed < totalProcessingTime && !shutdownHandler.isShuttingDown()) {
        try {
            Thread.sleep(EXTEND_INTERVAL * 1000);  // Simulate work
            elapsed += EXTEND_INTERVAL;
            System.out.println("Processing... " + elapsed + "/" + totalProcessingTime + " seconds");

            // Extend visibility timeout here
            sqs.changeMessageVisibility(ChangeMessageVisibilityRequest.builder()
                    .queueUrl(QUEUE_URL)
                    .receiptHandle(receiptHandle)
                    .visibilityTimeout(EXTEND_BY)
                    .build());

        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            System.err.println("Interrupted while processing");
            break;
        }
    }

    if (shutdownHandler.isShuttingDown()) {
        System.out.println("Graceful shutdown triggered. Skipping message deletion.");
    } else {
        System.out.println("Finished. Deleting message.");
        sqs.deleteMessage(DeleteMessageRequest.builder()
                .queueUrl(QUEUE_URL)
                .receiptHandle(receiptHandle)
                .build());
    }
}
```
Key Points:
| Element                       | Description                                                     |
| ----------------------------- | --------------------------------------------------------------- |
| `@PreDestroy`                 | Called automatically on SIGTERM in Spring Boot                  |
| `AtomicBoolean` shutdown flag | Shared across worker & background threads                       |
| Graceful loop check           | Every loop (`while`) checks `!shutdownHandler.isShuttingDown()` |
| Skips deletion                | Ensures we **don‚Äôt delete** the message if job was interrupted  |


PreStop Hook Snippet for Alerts
What is it?
The preStop hook gives the container a final notification before shutdown. This allows us to:

> Log shutdown intent clearly (which can be picked up by Splunk alerts).

> Set flags or drain messages before SIGKILL.

kuberentes yaml sample:

```
lifecycle:
  preStop:
    exec:
      command: ["/bin/sh", "-c", "echo 'Shutting down pod gracefully' >> /var/log/app/shutdown.log"]
```

How this helps app teams:

> Allows deterministic cleanup before termination.

> Logs from preStop can be monitored in Splunk for alerting on abnormal shutdowns.

> App teams get visibility and time to gracefully drain workloads.

terminationGracePeriodSeconds:
kubernetes yaml sample:
```
spec:
  terminationGracePeriodSeconds: 60
```
Why 60 seconds?

> Default is 30s, which might be insufficient sometimes for message processing or cleanup.

> 60s allows enough time for:

    >Message completion

    >Flushing logs

    >Resource cleanup

How this fits into workflow:

> Kubernetes sends SIGTERM.

> App receives signal and starts shutdown logic.

> App completes in-flight tasks or logs intent.

> If it exceeds terminationGracePeriodSeconds, Kubernetes forcefully sends SIGKILL.

We can configure this per deployment via Helm chart values:
helm sample yaml
```
terminationGracePeriodSeconds: {{ .Values.app.terminationGracePeriodSeconds | default 60 }}
```

Java / Spring Boot Example
In Java, particularly with Spring Boot, graceful shutdown is typically done using a @PreDestroy method or a shutdown hook.

Spring Boot SIGTERM Handling Example

```
import org.springframework.stereotype.Component;
import javax.annotation.PreDestroy;

@Component
public class GracefulShutdownManager {

    private volatile boolean shuttingDown = false;

    public boolean isShuttingDown() {
        return shuttingDown;
    }

    @PreDestroy
    public void onShutdown() {
        shuttingDown = true;
        System.out.println("SIGTERM received. Initiating graceful shutdown...");
        // Example: stop consuming new messages, finish current work
        // Avoid deleting messages from queue until processing completes
    }
}
```

Thread-safe Usage in Worker Loops
```
while (!shutdownManager.isShuttingDown()) {
    // Fetch message
    // Process safely
}
System.out.println("Graceful exit completed.");
```
Summary of Recommendations

| Item                            | Recommendation                                                | Benefit                      |
| ------------------------------- | ------------------------------------------------------------- | ---------------------------- |
| `preStop` hook                  | Log shutdown events                                           | Alerts, observability        |
| `terminationGracePeriodSeconds` | Set to 60 seconds                                             | Gives time for cleanup       |
| Signal handling                 | Use `@PreDestroy` or `Runtime.getRuntime().addShutdownHook()` | Safe termination             |
| Logging                         | Ensure shutdown logs are visible                              | Helps in debugging, alerting |
| Queue behavior                  | Avoid deleting message on shutdown                            | Prevent message loss         |




----------------------------------------------------------------------










4)Logging 
All logs must be written to stdout

Use structured log format:

```
2025-06-23T10:00:00Z | INFO | Processing message: {messageId: abc123}
2025-06-23T10:00:15Z | WARN | SIGTERM received, exiting
2025-06-23T10:00:20Z | ERROR | Job failed with: TimeoutError
```
Required fields:

message_id, job_id, retry_count, duration, status, error



-----------------------------------------------------------
4) Observability & Logging
Logging Best Practices
All logs must be written to stdout in a structured log format for compatibility with log aggregation tools like Splunk, CloudWatch, or ELK.

Recommended pattern:
<timestamp> | <level> | <message> | <contextual fields>

Example:
2025-06-23T10:00:00Z | INFO  | Started processing message            | message_id=abc123 job_id=job789 retry_count=1
2025-06-23T10:00:05Z | INFO  | Extending message visibility by 30s  | message_id=abc123 job_id=job789
2025-06-23T10:00:15Z | WARN  | SIGTERM received, shutting down      | job_id=job789
2025-06-23T10:00:20Z | ERROR | Job failed                           | message_id=abc123 job_id=job789 error=TimeoutException duration=12000ms

Required Fields in Logs
Make sure to include these fields in relevant log statements:

> message_id

> job_id

> retry_count

> duration (in milliseconds)

> status

> error (if applicable)

sample log snippets:
```
logger.info("Started processing message", kv("message_id", msgId), kv("job_id", jobId), kv("retry_count", retryCount));
logger.info("Extending message visibility by 30 seconds", kv("message_id", msgId), kv("job_id", jobId));
logger.warn("Shutdown signal received, processing will stop gracefully", kv("job_id", jobId));
logger.error("Job failed", kv("message_id", msgId), kv("job_id", jobId), kv("error", exception), kv("duration", duration));
```
Use a structured logger like logstash-logback-encoder, SLF4J with MDC, or similar for consistent key-value output.

Key Metrics to Track
Metrics help us understand performance and reliability trends over time.

| Metric                                        | Description                            |
| --------------------------------------------- | -------------------------------------- |
| **`processing_duration_ms`**                  | Time taken to process a message        |
| **`retry_count`**                             | Number of times a job was retried      |
| **`message_visibility_extensions`**           | Count of visibility extension requests |
| **`job_success_count` / `job_failure_count`** | Success/failure metrics                |
| **`sigterm_received_count`**                  | Tracks shutdown signals received       |

example metric Logging:
```
long duration = System.currentTimeMillis() - startTime;
logger.info("Finished processing", kv("message_id", msgId), kv("job_id", jobId), kv("duration", duration), kv("status", "success"));
```

Dashboards and Monitoring Recommendations
To ensure complete visibility into job performance and pod stability:

> OOMKilled Dashboard
Monitor memory usage trends and track containers terminated with OOMKilled status.
Recommended: Use Prometheus + Grafana or Cloud-native dashboards like GKE Workload Details.

> Job Failure Dashboard
Track:

  > Job error types (Timeout, Exception, etc.)

  > Frequency of retries

  > Duration before failure

  > Message IDs of failed jobs


---------------------------------------------------------------------


5) Keda Configurations:
This configuration defines how the KSJ sample app integrates with KEDA ScaledJob for scaling behavior.
These values are reference defaults for this sample pattern. Actual values should be adjusted based on your job‚Äôs behavior, retry policy, and cost/resource tradeoffs.

| Setting                       | Recommended Default             | Description                                                             |
| ----------------------------- | ------------------------------- | ------------------------------------------------------------------------|
| `pollingInterval`             | `10s`                           | Frequency at which KEDA checks SQS for pending messages                 |
| `minReplicaCount`             | `0`                             |Zero idle pods; scale from zero                                          |
| `maxReplicaCount`             | Set to match queue scale limits |Maximum parallel job pods allowed                                        |                                       |
| `scaledJob.concurrencyPolicy` | `Forbid`                        | Prevents more than one job processing the same message                  |
| `jobTargetRef.backoffLimit`   | `0`                             | Retries not needed ‚Äî let SQS redelivery handle failed jobs              |
|  `successfulJobsHistoryLimit` | `2`                             | Keeps last 2 completed jobs (adjust as needed for debugging/monitoring) |
| `failedJobsHistoryLimit`      | `2`                             | Keeps last 2 failed jobs (adjust as needed for debugging/monitoring)    |

Notes on Guidance
1) Set backoffLimit: 0 if your message is reprocessed by SQS ‚Äî this avoids Kubernetes retries that might conflict with message reappearance

2) Set successfulJobsHistoryLimit / failedJobsHistoryLimit based on debugging needs vs. resource cleanup

3) Use concurrencyPolicy: Forbid to avoid concurrent duplicate processing

4) Prefer the accurate scaling strategy for better control
```
apiVersion: keda.sh/v1alpha1
kind: ScaledJob
metadata:
  name: my-sqs-worker
spec:
  pollingInterval: 10
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 2
  maxReplicaCount: 10
  jobTargetRef:
    parallelism: 1
    backoffLimit: 0  # Let SQS handle retries instead of Kubernetes
  strategy:
    strategy: "accurate"
  triggers:
    - type: aws-sqs-queue
      metadata:
        queueURL: https://sqs.us-east-1.amazonaws.com/1234/my-queue
        awsRegion: us-east-1

```
Logging:
Use structured logs, ideally in JSON, for parsing in tools like Fluent Bit ‚Üí Splunk  / CloudWatch.
| Stage                       | Log Example / Message                                     |
| --------------------------- | --------------------------------------------------------- |
| Job Start                   | `"Starting KSJ job execution"`                            |
| SQS Polling                 | `"Polling SQS for message"`                               |
| Message Received            | `"Received message with ID: 123abc"`                      |
| Message Processing Started  | `"Started processing message: {...}"`                     |
| Visibility Timeout Extended | `"Extending visibility timeout by 30s"`                   |
| Processing Success          | `"Successfully processed message"`                        |
| Message Deleted             | `"Message deleted from SQS"`                              |
| Job Interrupted (SIGTERM)   | `"Received SIGTERM. Job interrupted."`                    |
| Exception / Failure         | `"Exception occurred during message processing: <error>"` |

Include contextual fields:

messageId, jobId, timestamp, status, queueName, durationMs, region

Metrics: use metrics for job health and throughput
| Metric                        | Use                                                                   |
| ----------------------------- | --------------------------------------------------------------------- |
| `job_start_total`             | Count of job starts                                                   |
| `job_success_total`           | Count of successful message processing                                |
| `job_failure_total`           | Count of failed jobs                                                  |
| `job_duration_seconds`        | Histogram of job processing time                                      |
| `visibility_extensions_total` | Number of visibility extensions (to catch anomalies or infinite jobs) |
